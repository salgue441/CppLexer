/**
 * @file lexer.cpp
 * @author Carlos Salguero
 * @author Sergio Garnica
 * @brief Implementation of the Lexer class
 * @version 0.1
 * @date 2023-05-12
 *
 * @copyright Copyright (c) 2023
 *
 */

// Standard libraries
#include <iostream>
#include <fstream>
#include <filesystem>

// Project files
#include "lexer.h"
#include "csharp_language.h"

// Access Methods
/**
 * @brief
 * Gets the tokens generated by the lexer
 * @return std::vector<Token> Vector of tokens
 */
std::vector<Token> Lexer::get_tokens() const
{
    return m_tokens;
}

// Methods (Public)
/**
 * @brief
 * Starts the lexing of the files
 * @param filenames Vector of filenames
 */
void Lexer::start_lexing(const std::vector<std::string> &filenames)
{
    try
    {
        for (const auto &filename : filenames)
        {
            auto tokens = lex_file(filename);
            save(filename, tokens);
        }
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Saves the tokens in a HTML file
 * @param filename Filename to save the tokens
 * @throw std::runtime_error If the file cannot be opened
 */
void Lexer::save(const std::string &filename, const std::vector<Token> &tokens) const
{
    try
    {
        std::string output_filename = get_output_filename(filename);
        std::ofstream output_file(output_filename);
        if (!output_file)
        {
            throw std::runtime_error("Cannot open file: " + output_filename);
        }

        output_file << generate_html(tokens);
        output_file.close();
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

// Methods (Private)
/**
 * @brief
 * Lex the files with parallel threads
 * @param filenames Vector of filenames
 */
void Lexer::lex(const std::vector<std::string> &filenames)
{
    try
    {
        for (const auto &filename : filenames)
            m_threads.emplace_back(&Lexer::lex_file, this,
                                   filename);

        for (auto &thread : m_threads)
            thread.join();
    }

    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Lex a file and generate the tokens for said file
 * @param filename Filename to lex
 * @throw std::runtime_error If the file cannot be opened
 */
std::vector<Token> Lexer::lex_file(const std::string_view &filename)
{
    try
    {
        std::ifstream input_file(filename.data(), std::ios::in | std::ios::binary);
        if (!input_file)
        {
            throw std::runtime_error("Cannot open file: " + std::string(filename));
        }

        std::string buffer((std::istreambuf_iterator<char>(input_file)),
                           std::istreambuf_iterator<char>());

        input_file.close();

        if (buffer.empty())
        {
            throw std::runtime_error("File is empty: " + std::string(filename));
        }

        return tokenize(buffer);
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Tokenizes the source code and generates the html code
 * @param buffer Source code to tokenize
 * @return std::vector<Token> Vector of html code
 * @throw std::runtime_error If the file cannot be opened
 */
std::vector<Token> Lexer::tokenize(const std::string_view &buffer)
{
    try
    {
        std::vector<Token> tokens;
        std::string_view delimiter{" \t\n\r\f\v"};
        std::size_t position{};
        std::size_t previous_position{};

        while ((position = buffer.find_last_of(delimiter, position)) != std::string::npos)
        {
            std::string_view token(buffer.data() + previous_position,
                                   position - previous_position);
            TokenType type = identify_token(token);
            tokens.emplace_back(token.data(), type);

            previous_position = position + 1;
            ++position;
        }

        if (previous_position < buffer.size())
        {
            std::string_view token(buffer.data() + previous_position,
                                   buffer.size() - previous_position);
            TokenType type = identify_token(token);
            tokens.emplace_back(token.data(), type);
        }

        return tokens;
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }

    throw std::runtime_error("Cannot tokenize file");
}

/**
 * @brief
 * Identify the token type based of the Token class
 * @param token Token to identify
 * @return TokenType Type of the token
 */
TokenType Lexer::identify_token(const std::string_view &token)
{
    if (std::binary_search(csharp::m_keywords.begin(),
                           csharp::m_keywords.end(), token))
        return TokenType::Keyword;

    else if (std::binary_search(csharp::m_operators.begin(),
                                csharp::m_operators.end(), token))
        return TokenType::Operator;

    else if (std::binary_search(csharp::m_separators.begin(),
                                csharp::m_separators.end(), token))
        return TokenType::Separator;

    else if (std::binary_search(csharp::m_comments.begin(),
                                csharp::m_comments.end(), token))
        return TokenType::Comment;

    else if (std::binary_search(csharp::m_literals.begin(),
                                csharp::m_literals.end(), token))
        return TokenType::Literal;

    else if (std::binary_search(csharp::m_preprocessor.begin(),
                                csharp::m_preprocessor.end(), token))
        return TokenType::Preprocessor;

    else if (std::binary_search(csharp::m_contextual_keywords.begin(),
                                csharp::m_contextual_keywords.end(),
                                token))
        return TokenType::ContextualKeyword;

    else if (std::binary_search(csharp::m_access_specifiers.begin(),
                                csharp::m_access_specifiers.end(),
                                token))
        return TokenType::AccessSpecifier;

    else if (std::binary_search(csharp::m_attribute_targets.begin(),
                                csharp::m_attribute_targets.end(),
                                token))
        return TokenType::AttributeTarget;

    else if (std::binary_search(csharp::m_attribute_usage.begin(),
                                csharp::m_attribute_usage.end(),
                                token))
        return TokenType::AttributeUsage;

    else if (std::binary_search(csharp::m_escaped_identifiers.begin(),
                                csharp::m_escaped_identifiers.end(),
                                token))
        return TokenType::EscapedIdentifier;

    else if (std::binary_search(csharp::m_interpolated_strings.begin(),
                                csharp::m_interpolated_strings.end(), token))
        return TokenType::InterpolatedStringLiteral;

    else if (std::binary_search(csharp::m_nullables.begin(),
                                csharp::m_nullables.end(), token))
        return TokenType::NullLiteral;

    else if (std::binary_search(csharp::m_verbatim_strings.begin(),
                                csharp::m_verbatim_strings.end(),
                                token))
        return TokenType::VerbatimStringLiteral;

    return TokenType::Other;
}

/**
 * @brief
 * Converts the tokens to html code. Uses the style.css defined classes
 * to color the tokens
 * @param tokens Tokens to convert
 * @return std::string Html code
 */
std::string Lexer::token_to_html(const Token &token) const
{
    std::string html;

    switch (token.get_type())
    {
    case TokenType::Keyword:
        html += "<span class=\"Keyword\">" + token.get_value() + "</span>";
        break;

    case TokenType::Identifier:
        html += "<span class=\"Identifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::Literal:
        html += "<span class=\"Literal\">" + token.get_value() + "</span>";
        break;

    case TokenType::Operator:
        html += "<span class=\"Operator\">" + token.get_value() + "</span>";
        break;

    case TokenType::Separator:
        html += "<span class=\"Separator\">" + token.get_value() + "</span>";
        break;

    case TokenType::Comment:
        html += "<span class=\"Comment\">" + token.get_value() + "</span>";
        break;

    case TokenType::Preprocessor:
        html += "<span class=\"Preprocessor\">" + token.get_value() + "</span>";
        break;

    case TokenType::ContextualKeyword:
        html += "<span class=\"ContextualKeyword\">" + token.get_value() + "</span>";
        break;

    case TokenType::AccessSpecifier:
        html += "<span class=\"AccessSpecifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::AttributeTarget:
        html += "<span class=\"AttributeTarget\">" + token.get_value() + "</span>";
        break;

    case TokenType::AttributeUsage:
        html += "<span class=\"AttributeUsage\">" + token.get_value() + "</span>";
        break;

    case TokenType::EscapedIdentifier:
        html += "<span class=\"EscapedIdentifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::InterpolatedStringLiteral:
        html += "<span class=\"InterpolatedStringLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::NullLiteral:
        html += "<span class=\"NullLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::VerbatimStringLiteral:
        html += "<span class=\"VerbatimStringLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::RegularExpressionLiteral:
        html += "<span class=\"RegularExpressionLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::NumericLiteral:
        html += "<span class=\"NumericLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::Other:
        html += "<span class=\"Other\">" + token.get_value() + "</span>";
        break;
    }

    return html;
}

/**
 * @brief
 * Generates the HTML code from the tokens vector
 * @param tokens Tokens to convert
 * @return std::string Html code
 */
std::string Lexer::generate_html(const std::vector<Token> &tokens) const
{
    std::string html;

    html += "<!DOCTYPE html>\n";
    html += "<html lang=\"en\">\n";
    html += "<head>\n";
    html += "<meta charset=\"UTF-8\">\n";
    html += "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n";
    html += "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n";
    html += "<title>Highlighter</title>\n";
    html += "<link rel=\"stylesheet\" href=\"style.css\">\n";
    html += "</head>\n";
    html += "<body>\n";
    html += "<pre><code>\n";

    for (const auto &token : tokens)
    {
        html += token_to_html(token);
    }

    html += "</code></pre>\n";
    html += "</body>\n";
    html += "</html>\n";

    return html;
}

/**
 * @brief
 * Gets the output filename from the input filename
 * @param inputFilename Input filename
 * @return std::string Output filename
 */
std::string Lexer::get_output_filename(const std::string &inputFilename) const
{
    std::filesystem::path path(inputFilename);
    std::string filename = path.stem().string();
    std::filesystem::path outputPath = "../output/" + filename + ".html";

    return outputPath.string();
}