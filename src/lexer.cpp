/**
 * @file lexer.cpp
 * @author Carlos Salguero
 * @author Sergio Garnica
 * @brief Implementation of the Lexer class
 * @version 0.1
 * @date 2023-05-12
 *
 * @copyright Copyright (c) 2023
 *
 */

// Standard libraries
#include <iostream>
#include <fstream>
#include <filesystem>

// Project files
#include "lexer.h"
#include "csharp_language.h"

// Regex
std::regex Lexer::m_regex_tokenizer(R"([a-zA-Z0-9][a-zA-Z0-9_]+|\s+|\{|\}|\(|\)|\[|\]|\;|\,|\.\:\?\>\<\+\-\*\/\%\^\&\=\!\@\#\$\~\,\_\`\\])");

// Access Methods
/**
 * @brief
 * Gets the tokens generated by the lexer
 * @return std::vector<Token> Vector of tokens
 */
std::vector<Token> Lexer::get_tokens() const
{
    return m_tokens;
}

// Methods (Public)
/**
 * @brief
 * Starts the lexing of the files
 * @param filenames Vector of filenames
 */
void Lexer::start_lexing(const std::vector<std::string> &filenames)
{
    try
    {
        for (const auto &filename : filenames)
        {
            auto tokens = lex_file(filename);
            save(filename, tokens);
        }
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Saves the tokens in a HTML file
 * @param filename Filename to save the tokens
 * @throw std::runtime_error If the file cannot be opened
 */
void Lexer::save(const std::string &filename, const std::vector<Token> &tokens) const
{
    try
    {
        std::string output_filename = get_output_filename(filename);
        std::ofstream output_file(output_filename);
        if (!output_file)
        {
            throw std::runtime_error("Cannot open file: " + output_filename);
        }

        output_file << generate_html(tokens);
        output_file.close();
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

// Methods (Private)
/**
 * @brief
 * Lex the files with parallel threads
 * @param filenames Vector of filenames
 */
void Lexer::lex(const std::vector<std::string> &filenames)
{
    try
    {
        for (const auto &filename : filenames)
            m_threads.emplace_back(&Lexer::lex_file, this,
                                   filename);

        for (auto &thread : m_threads)
            thread.join();
    }

    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Lex a file and generate the tokens for said file
 * @param filename Filename to lex
 * @throw std::runtime_error If the file cannot be opened
 */
std::vector<Token> Lexer::lex_file(const std::string_view &filename)
{
    try
    {
        std::ifstream input_file(filename.data(),
                                 std::ios::in | std::ios::binary);

        if (!input_file)
        {
            throw std::runtime_error("Cannot open file: " + std::string(filename));
        }

        std::string buffer((std::istreambuf_iterator<char>(input_file)),
                           std::istreambuf_iterator<char>());

        input_file.close();

        if (buffer.empty())
            throw std::runtime_error("File is empty: " + std::string(filename));

        return tokenize(buffer);
    }
    catch (std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Tokenizes the source code and generates the html code.
 * Uses regex to identify the m_separator tokens in order to highlight
 * them in the html code.
 * @param buffer Source code to tokenize
 * @return std::vector<Token> Vector of html code
 * @throw std::runtime_error If the file cannot be opened
 */
std::vector<Token> Lexer::tokenize(const std::string_view &buffer)
{
    try
    {
        std::vector<Token> tokens;
        std::string buffer_str(buffer);

        auto token_begin = std::sregex_token_iterator(
            buffer_str.begin(), buffer_str.end(),
            m_regex_tokenizer);

        const auto token_end = std::sregex_token_iterator();

        for (auto it{token_begin}; it != token_end; ++it)
        {
            const std::string token = it->str();

            if (!token.empty())
            {
                TokenType token_type = identify_token(token);
                tokens.emplace_back(token, token_type);
            }
        }

        return tokens;
    }
    catch (const std::exception &e)
    {
        throw std::runtime_error(e.what());
    }
}

/**
 * @brief
 * Creates the unordered map with the csharp constexpr arrays
 * @return std::unordered_map<std::string, std::string> Unordered map with the csharp constexpr arrays
 */
std::unordered_map<std::string_view, TokenType> Lexer::create_token_map() const
{
    std::unordered_map<std::string_view, TokenType> token_map;

    // Reserve space for the unordered map
    std::size_t size = csharp::m_keywords.size() +
                       csharp::m_operators.size() +
                       csharp::m_separators.size() +
                       csharp::m_comments.size() +
                       csharp::m_literals.size() +
                       csharp::m_preprocessor.size() +
                       csharp::m_contextual_keywords.size() +
                       csharp::m_access_specifiers.size() +
                       csharp::m_attribute_targets.size() +
                       csharp::m_attribute_usage.size() +
                       csharp::m_escaped_identifiers.size() +
                       csharp::m_interpolated_strings.size() +
                       csharp::m_nullables.size() +
                       csharp::m_verbatim_strings.size();

    token_map.reserve(size);

    /**
     * @brief
     * Inserts the elements of a container into the unordered map
     * @param container Container to insert
     * @param type TokenType of the container
     * @return void
     */
    auto insert_range = [&](const auto &container, TokenType type)
    {
        for (const auto &item : container)
            token_map.emplace_hint(token_map.end(), item, type);
    };

    insert_range(csharp::m_keywords, TokenType::Keyword);
    insert_range(csharp::m_operators, TokenType::Operator);
    insert_range(csharp::m_separators, TokenType::Separator);
    insert_range(csharp::m_comments, TokenType::Comment);
    insert_range(csharp::m_literals, TokenType::Literal);
    insert_range(csharp::m_preprocessor,
                 TokenType::Preprocessor);
    insert_range(csharp::m_contextual_keywords,
                 TokenType::ContextualKeyword);
    insert_range(csharp::m_access_specifiers,
                 TokenType::AccessSpecifier);
    insert_range(csharp::m_attribute_targets,
                 TokenType::AttributeTarget);
    insert_range(csharp::m_attribute_usage,
                 TokenType::AttributeUsage);
    insert_range(csharp::m_escaped_identifiers,
                 TokenType::EscapedIdentifier);
    insert_range(csharp::m_interpolated_strings,
                 TokenType::InterpolatedStringLiteral);
    insert_range(csharp::m_nullables, TokenType::NullLiteral);
    insert_range(csharp::m_verbatim_strings, TokenType::VerbatimStringLiteral);

    return token_map;
}

/**
 * @brief
 * Identify the token type based of the Token class
 * @param token Token to identify
 * @return TokenType Type of the token
 */
TokenType Lexer::identify_token(const std::string_view &token)
{
    static const std::unordered_map<std::string_view, TokenType>
        token_map = create_token_map();
    const auto it = token_map.find(token);

    if (it != token_map.end())
        return it->second;

    return TokenType::Other;
}

/**
 * @brief
 * Converts the tokens to html code. Uses the style.css defined classes
 * to color the tokens
 * @param tokens Tokens to convert
 * @return std::string Html code
 */
std::string Lexer::token_to_html(const Token &token) const
{
    std::string html;

    switch (token.get_type())
    {
    case TokenType::Keyword:
        html += "<span class=\"Keyword\">" + token.get_value() + "</span>";
        break;

    case TokenType::Identifier:
        html += "<span class=\"Identifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::Literal:
        html += "<span class=\"Literal\">" + token.get_value() + "</span>";
        break;

    case TokenType::Operator:
        html += "<span class=\"Operator\">" + token.get_value() + "</span>";
        break;

    case TokenType::Separator:
        html += "<span class=\"Separator\">" + token.get_value() + "</span>";
        break;

    case TokenType::Comment:
        html += "<span class=\"Comment\">" + token.get_value() + "</span>";
        break;

    case TokenType::Preprocessor:
        html += "<span class=\"Preprocessor\">" + token.get_value() + "</span>";
        break;

    case TokenType::ContextualKeyword:
        html += "<span class=\"ContextualKeyword\">" + token.get_value() + "</span>";
        break;

    case TokenType::AccessSpecifier:
        html += "<span class=\"AccessSpecifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::AttributeTarget:
        html += "<span class=\"AttributeTarget\">" + token.get_value() + "</span>";
        break;

    case TokenType::AttributeUsage:
        html += "<span class=\"AttributeUsage\">" + token.get_value() + "</span>";
        break;

    case TokenType::EscapedIdentifier:
        html += "<span class=\"EscapedIdentifier\">" + token.get_value() + "</span>";
        break;

    case TokenType::InterpolatedStringLiteral:
        html += "<span class=\"InterpolatedStringLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::NullLiteral:
        html += "<span class=\"NullLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::VerbatimStringLiteral:
        html += "<span class=\"VerbatimStringLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::RegularExpressionLiteral:
        html += "<span class=\"RegularExpressionLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::NumericLiteral:
        html += "<span class=\"NumericLiteral\">" + token.get_value() + "</span>";
        break;

    case TokenType::Other:
        html += "<span class=\"Other\">" + token.get_value() + "</span>";
        break;
    }

    return html;
}

/**
 * @brief
 * Generates the HTML code from the tokens vector
 * @param tokens Tokens to convert
 * @return std::string Html code
 */
std::string Lexer::generate_html(const std::vector<Token> &tokens) const
{
    std::stringstream html;

    html << "<!DOCTYPE html>\n";
    html << "<html lang=\"en\">\n";
    html << "<head>\n";
    html << "<meta charset=\"UTF-8\">\n";
    html << "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n";
    html << "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n";
    html << "<title>Highlighter</title>\n";
    html << "<link rel=\"stylesheet\" href=\"../src/styles/styles.css\">\n";
    html << "</head>\n";
    html << "<body style=\"background-color: var(--background-color);\">\n";
    html << "<pre><code>\n";

    int indentationLevel = 0;
    std::string indentation(indentationLevel * 2, ' ');
    bool previousTokenWasNewLine = false;

    auto addIndentation = [&]()
    {
        html << '\n'
             << indentation;
    };

    auto addToken = [&](const Token &token)
    {
        html << token_to_html(token);
    };

    for (const auto &token : tokens)
    {
        if (token.get_type() == TokenType::Separator &&
            token.get_value() == ";")
        {
            addIndentation();
            previousTokenWasNewLine = true;
        }
        else if (previousTokenWasNewLine && token.get_type() == TokenType::Separator && token.get_value() == ";")
        {
            html << token_to_html(token) << '\n'
                 << indentation;
            previousTokenWasNewLine = true;
            continue;
        }
        else if (token.get_type() == TokenType::Separator &&
                 token.get_value() == "\t")
        {
            html << "&emsp;";
            previousTokenWasNewLine = false;
        }
        else if (token.get_type() == TokenType::Separator &&
                 token.get_value() == " ")
        {
            html << "&nbsp;";
            previousTokenWasNewLine = false;
        }
        else if (token.get_type() == TokenType::Comment)
        {
            html << "<br>";
            addIndentation();
            html << token_to_html(token) << "<br>";
            addIndentation();
            previousTokenWasNewLine = true;
        }
        else
        {
            addToken(token);
            previousTokenWasNewLine = false;
        }

        if (token.get_type() == TokenType::Separator &&
            token.get_value() == "{")
        {
            addIndentation();
            indentationLevel++;
            indentation = std::string(indentationLevel * 2, ' ');
            previousTokenWasNewLine = true;
        }
        else if (token.get_type() == TokenType::Separator &&
                 token.get_value() == "}")
        {
            addIndentation();
            indentationLevel--;
            indentation = std::string(indentationLevel * 2, ' ');
            previousTokenWasNewLine = true;
        }
    }

    html << "</code></pre>\n";
    html << "</body>\n";
    html << "</html>\n";

    return html.str();
}

/**
 * @brief
 * Gets the output filename from the input filename
 * @param inputFilename Input filename
 * @return std::string Output filename
 */
std::string Lexer::get_output_filename(const std::string &inputFilename) const
{
    std::filesystem::path path(inputFilename);
    std::string filename = path.stem().string();
    std::filesystem::path outputPath =
        "../outputParallel/" + filename + ".html";

    return outputPath.string();
}